#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{pgfplots}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman lmodern
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\columnsep 0.2in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Nearest Neighbor Algorithms for Text Classification
\end_layout

\begin_layout Author
Steven Xu (350256)
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $k$
\end_inset

-nearest-neighbor algorithm is a method for classification, where a test
 instance is classified by using the majority vote of the 
\begin_inset Formula $k$
\end_inset

 closest training examples in the feature space.
\end_layout

\begin_layout Standard
\begin_inset Formula $k$
\end_inset

-nearest-neighbors can in particular be used for text classification, where
 it can handle multiple classes directly, and achieves good effectiveness
 comparable but less than that of support vector machines.
\end_layout

\begin_layout Standard
A linear search has high computational time and memory requirements when
 there are a large number of documents to search through.
 We give a review of some more efficient algorithms, and compare them for
 their effectiveness on text classification.
\end_layout

\begin_layout Section
Algorithms
\end_layout

\begin_layout Standard
A common approach to search for exact nearest neighbors is by organizing
 points in the database into a space partitioning tree, then using branch
 and bound to prune portions of the tree which are known to be non-optimal.
 Algorithms include the k-dimensional (kd) tree 
\begin_inset CommandInset citation
LatexCommand cite
key "sproull_refinements_1991"

\end_inset

, the principal axis (pa) tree 
\begin_inset CommandInset citation
LatexCommand cite
key "mcnames_fast_2001"

\end_inset

, and the vantage point (vp) tree 
\begin_inset CommandInset citation
LatexCommand cite
key "yianilos_data_1993"

\end_inset

.
\end_layout

\begin_layout Standard
However algorithms depend on a low intrinsic dimension of the dataset, as
 for sufficiently high dimensions none are better than linear search 
\begin_inset CommandInset citation
LatexCommand cite
key "weber_quantitative_1998"

\end_inset

.
\end_layout

\begin_layout Standard
We describe these algorithms for finding the single nearest point.
 It is easy to find multiple nearest points by using a max-heap to store
 the best distances, then using the maximum distance in the heap for comparison
 during search.
\end_layout

\begin_layout Subsection
KD Tree
\end_layout

\begin_layout Standard
A kd tree is a space partitioning binary tree, with an axis selected at
 each node to partition it's set of points.
 There are many ways to choose the axis, the simplest way is to cycle it
 amongst the dimensions of the dataset, described below.
\end_layout

\begin_layout Subsubsection
Tree Construction
\end_layout

\begin_layout Standard
We start from the root node of the tree with the entire dataset of points,
 and select the first dimension of the dataset as the axis to partition
 the tree.
 The pivot is selected as the median of all points in the dataset, projected
 on the axis.
 The space is then partitioned in two, the points to the left and right
 of the pivot, assigned to the left and right child nodes.
 For each child 
\begin_inset Formula $i$
\end_inset

, we also store the minimum distance a projected point has to the pivot,
 
\begin_inset Formula $lb_{i}$
\end_inset

, to aid in branch-and-bound search.
\end_layout

\begin_layout Standard
The process is then repeated for each child, with the axis selected as the
 one after the parent's axis.
 We continue until no points remain.
\end_layout

\begin_layout Standard
Each node stores the axis used, the pivot point, and the minimum distances
 to the pivot of the left and right child.
\end_layout

\begin_layout Subsubsection
Tree Search
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $q$
\end_inset

 be the query point.
 The distance between 
\begin_inset Formula $q$
\end_inset

 and the root node's pivot is computed, and update this as the current best
 distance 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Standard
For each child node 
\begin_inset Formula $i$
\end_inset

, we check whether the points in it can be closer to 
\begin_inset Formula $q$
\end_inset

 than 
\begin_inset Formula $d$
\end_inset

, by intersecting a hypersphere 
\begin_inset Formula $S(q,d)$
\end_inset

 with the region's separating hyperplane.
 The distance between 
\begin_inset Formula $q$
\end_inset

 and the hyperplane can be calculated by projecting the query point on the
 axis, computing it's distance to the pivot point, then adding 
\begin_inset Formula $lb_{i}$
\end_inset

.
 We have no intersections if this is greater than 
\begin_inset Formula $d$
\end_inset

, therefore the node can be pruned.
\end_layout

\begin_layout Standard
This is repeated until we have searched or eliminated all nodes.
\end_layout

\begin_layout Subsection
Principal Axis Tree
\end_layout

\begin_layout Standard
A pa tree is similar to that of a kd tree, where the axis are selected to
 be the principal components of the dataset.
 This splits the data amongst the axis of most variance into a predefined
 
\begin_inset Formula $n_{c}$
\end_inset

 number of regions, to try to adapt and cluster the points in the dataset.
\end_layout

\begin_layout Subsubsection
Tree Construction
\end_layout

\begin_layout Standard
PCA is first performed on the dataset to compute the principal axis.
\end_layout

\begin_layout Standard
Each point in the dataset is then projected onto the first principal axis,
 and split evenly into 
\begin_inset Formula $n_{c}$
\end_inset

 regions, with the left and right separating hyperplanes for each region
 stored as projected points.
 This process is continued recursively until we have less than 
\begin_inset Formula $n_{c}$
\end_inset

 points in a node, where it then becomes a terminal node.
\end_layout

\begin_layout Standard
Each node stores the principal axis, and the left and right separating hyperplan
es for each region.
\end_layout

\begin_layout Subsubsection
Tree Search
\end_layout

\begin_layout Standard
The depth-first search begins by projecting the query point onto the first
 principal axis, with a binary search to determine which region it is in.
 This process is repeated for child region until we reach a terminal node,
 where we compute the distance and store it as the current best.
 The algorithm then moves onto the parent node, where the elimination criterion
 is applied to the siblings of the terminal node.
\end_layout

\begin_layout Standard
This continues until we have searched or eliminated all nodes.
\end_layout

\begin_layout Standard
Regions closer to the one the query point is in are therefore visited first,
 to arrive at a smaller distance sooner.
\end_layout

\begin_layout Standard
The elimination criterion for the root regions compares it's closest separating
 hyperplane's distance to the query point, and the current best distance,
 same as a kd tree.
 For a child region 
\begin_inset Formula $r_{n}$
\end_inset

 nested in parent regions 
\begin_inset Formula $r_{n-1},r_{n-2},\dots,r_{1}$
\end_inset

, it can be proved, using the law of cosines 
\begin_inset CommandInset citation
LatexCommand cite
key "mcnames_fast_2001"

\end_inset

, that for any point 
\begin_inset Formula $x\in r_{n}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
d(q,x)^{2}\ge d(q,r_{1})^{2}+d(b_{2},r_{2})^{2}+\dots+d(b_{n},r_{n})^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $d(q,r_{i})^{2}$
\end_inset

 is the distance between 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $r_{i}$
\end_inset

's closest separating hyperplane, and 
\begin_inset Formula $b_{2}$
\end_inset

 is 
\begin_inset Formula $q$
\end_inset

 projected onto 
\begin_inset Formula $r_{1}$
\end_inset

's closest separating hyerplane, 
\begin_inset Formula $b_{3}$
\end_inset

 is 
\begin_inset Formula $b_{2}$
\end_inset

 projected onto 
\begin_inset Formula $r_{2}$
\end_inset

's separating hyperplane, and so forth.
\end_layout

\begin_layout Standard
If this inequality is not satisfied, the region can be pruned.
 For efficiency, we only need to compute the squared distance to every region.
\end_layout

\begin_layout Subsection
Vantage Point Tree
\end_layout

\begin_layout Standard
Both the kd tree and the pa tree require Euclidean distances.
 The vantage point tree is a binary tree which can work with any metric.
 Cosine distance in particular can be turned into a metric.
\end_layout

\begin_layout Standard
Instead of using hyerplanes to partition the data, vantage point trees use
 spheres around a vantage point to partition data.
\end_layout

\begin_layout Standard
It has the theoretical property that a data distribution satisfying the
 Zero Point Spheres (ZPS) property (the probability of points being on a
 sphere is zero), and with a small enough distance bound, the nearest neighbor
 can be expected to be computed in 
\begin_inset Formula $O(log(n))$
\end_inset

 time with a database of size 
\begin_inset Formula $n$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "yianilos_data_1993"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Tree Construction
\end_layout

\begin_layout Standard
Starting from the root node, a vantage point 
\begin_inset Formula $p$
\end_inset

 is chosen amongst the points in the database 
\begin_inset Formula $S$
\end_inset

.
 This can be random for speed, but there can be other criterions to ensure
 that the tree is more balanced 
\begin_inset CommandInset citation
LatexCommand cite
key "yianilos_data_1993"

\end_inset

.
\end_layout

\begin_layout Standard
We then define a sphere with radius 
\begin_inset Formula $\mu=Median_{s\in S}d(p,s)$
\end_inset

.
 The points in the node are partitioned in two, with the left child being
 inside and right child outside or on the sphere.
 This can be done by computing the distances between each point and 
\begin_inset Formula $p$
\end_inset

 and 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mu$
\end_inset

.
 For each partition, upper and lower distance bounds to 
\begin_inset Formula $p$
\end_inset

 are computed.
 This is performed recursively for each child node.
\end_layout

\begin_layout Standard
Each node stores 
\begin_inset Formula $p$
\end_inset

, 
\begin_inset Formula $\mu$
\end_inset

, and the bounds.
\end_layout

\begin_layout Subsubsection
Tree Search
\end_layout

\begin_layout Standard
The distance between the root node's pivot and the query is stored as the
 current best.
 The node visiting criterion then prefers to visit the left node if this
 is closer to the left child, or right node otherwise.
 A node is visited if the distance is within it's bounds.
\end_layout

\begin_layout Standard
This is repeated until we have searched or eliminated all nodes.
\end_layout

\begin_layout Section
Experimental Setup
\end_layout

\begin_layout Standard
30,000 documents were used from the RCV1v2 document collection.
 To pre-process each document, the bag of words model was used with unit-length
 normalized tf-idf weights using the 
\begin_inset Formula $l_{2}$
\end_inset

 metric, with the following formulas:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
tf_{d,t} & =log(1+f_{d,t})\\
idf_{t} & =log(\frac{N}{f_{t}})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $f_{d,t}$
\end_inset

 is the number of times term 
\begin_inset Formula $t$
\end_inset

 appears in the document 
\begin_inset Formula $d$
\end_inset

; 
\begin_inset Formula $f_{t}$
\end_inset

is the number of documents term 
\begin_inset Formula $t$
\end_inset

 appears in throughout the collection; and 
\begin_inset Formula $N$
\end_inset

 is the number of documents in the collection.
\end_layout

\begin_layout Standard
Euclidean distance was used as the distance function, which is rank-equivalent
 to cosine similarity due to unit-length normalization.
\end_layout

\begin_layout Standard
To compute the principal axes, power iteration was used with a maximum of
 1,000 iterations.
\end_layout

\begin_layout Standard
We implemented all of the above algorithms in python.
 Due to python's performance limits compared to the linear search's C implementa
tion, we evaluated query performance based on the number of distances computed
 instead of the total time spent.
\end_layout

\begin_layout Standard
Partial distance search were not used due to it's negligble impact on performanc
e.
\end_layout

\begin_layout Section
Experimental Results
\end_layout

\begin_layout Subsection
Query Performance
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
begin{axis}[
\end_layout

\begin_layout Plain Layout

	width=220pt,
\end_layout

\begin_layout Plain Layout

	legend style={legend pos=north west,font=
\backslash
tiny},
\end_layout

\begin_layout Plain Layout

	xlabel={Database Size (log scale)},
\end_layout

\begin_layout Plain Layout

	ylabel={Average Nodes Visited},
\end_layout

\begin_layout Plain Layout

	xmode=log,
\end_layout

\begin_layout Plain Layout

	legend cell align=left,
\end_layout

\begin_layout Plain Layout

]
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=o] table [x={ndocs}, y={brute}] {ndocs_nodes.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=x] table [x={ndocs}, y={kdt}] {ndocs_nodes.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=square] table [x={ndocs}, y={pat_10}] {ndocs_nodes.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=triangle] table [x={ndocs}, y={pat_30}] {ndocs_nodes.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=diamond] table [x={ndocs}, y={vpt}] {ndocs_nodes.dat};
\end_layout

\begin_layout Plain Layout


\backslash
legend{linear search,kdt,pat ($n_c=10$),pat ($n_c=30$),vpt}
\end_layout

\begin_layout Plain Layout


\backslash
end{axis}
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Database size vs.
 average nodes visited for 50 random test documents.
 kd trees were no better than a linear search.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
begin{axis}[
\end_layout

\begin_layout Plain Layout

	width=220pt,
\end_layout

\begin_layout Plain Layout

	legend style={legend pos=south east,font=
\backslash
tiny},
\end_layout

\begin_layout Plain Layout

	xlabel={Minimum Distance},
\end_layout

\begin_layout Plain Layout

	ylabel={Nodes Traversed},
\end_layout

\begin_layout Plain Layout

	legend cell align=left,
\end_layout

\begin_layout Plain Layout

]
\end_layout

\begin_layout Plain Layout


\backslash
addplot[only marks,mark=square] table [x={dist}, y={pat_10}] {nodes_dist.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[only marks,mark=triangle] table [x={dist}, y={pat_30}] {nodes_dist.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[only marks,mark=diamond] table [x={dist}, y={vpt}] {nodes_dist.dat};
\end_layout

\begin_layout Plain Layout


\backslash
legend{pat ($n_c=10$),pat ($n_c=30$),vpt}
\end_layout

\begin_layout Plain Layout


\backslash
end{axis}
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Nodes traversed vs minimum distance for all test points, for a database
 of size 20000.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
None of the algorithms appear to exhibit logarithmic query time, but both
 vp trees and pa trees performed better than the naive algorithm.
 The performance benefit increases the smaller the minimum distance is.
\end_layout

\begin_layout Subsubsection
KD Tree
\end_layout

\begin_layout Standard
The kd tree was expected to perform poorly due to the high dimensionality
 of the data.
 This is since it reduces the effectiveness of the bound 
\begin_inset Formula $lb_{i}$
\end_inset

, which only involves a single dimension.
\end_layout

\begin_layout Standard
But it did not perform better at all compared to linear search.
 This can possibly be explained by the large number of dimensions involving
 rare words, which leads to the two partitions being highly unbalanced,
 therefore a highly unbalanced search tree.
 Since rare words also have low weights, this decreases 
\begin_inset Formula $lb_{i}$
\end_inset

 further.
\end_layout

\begin_layout Subsubsection
Principal Axis Tree
\end_layout

\begin_layout Standard
Better choice of axes compared to the kd-tree leads to better efficiency.
 A smaller 
\begin_inset Formula $n_{c}$
\end_inset

 also appears to be better.
\end_layout

\begin_layout Standard
Despite the lowest average query times, pa trees showed greater variance.
 In particular, there were 42% of test documents where all leaf nodes were
 visited for 
\begin_inset Formula $n_{c}=10$
\end_inset

, and 64% for 
\begin_inset Formula $n_{c}=30$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Vantage Point Tree
\end_layout

\begin_layout Standard
ZPS is not true, but is true for most points 
\end_layout

\begin_layout Subsection
Tree Construction
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
begin{axis}[
\end_layout

\begin_layout Plain Layout

	width=220pt,
\end_layout

\begin_layout Plain Layout

	legend style={legend pos=north west,font=
\backslash
tiny},
\end_layout

\begin_layout Plain Layout

	xlabel={Database Size},
\end_layout

\begin_layout Plain Layout

	ylabel={Time Spent (s)},
\end_layout

\begin_layout Plain Layout

	legend cell align=left,
\end_layout

\begin_layout Plain Layout

]
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=x] table [x={ndocs}, y={kdt}] {ndocs_time.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=square] table [x={ndocs}, y={pat_10}] {ndocs_time.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=triangle] table [x={ndocs}, y={pat_30}] {ndocs_time.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=diamond] table [x={ndocs}, y={vpt}] {ndocs_time.dat};
\end_layout

\begin_layout Plain Layout


\backslash
legend{kdt,pat ($n_c=10$),pat ($n_c=30$),vpt}
\end_layout

\begin_layout Plain Layout


\backslash
end{axis}
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Tree construction CPU time.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
begin{axis}[
\end_layout

\begin_layout Plain Layout

	width=220pt,
\end_layout

\begin_layout Plain Layout

	legend style={legend pos=north west,font=
\backslash
tiny},
\end_layout

\begin_layout Plain Layout

	xlabel={Database Size},
\end_layout

\begin_layout Plain Layout

	ylabel={Memory Used (bytes)},
\end_layout

\begin_layout Plain Layout

	legend cell align=left,
\end_layout

\begin_layout Plain Layout

]
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=x] table [x={ndocs}, y={kdt}] {ndocs_memory.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=square] table [x={ndocs}, y={pat_10}] {ndocs_memory.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=triangle] table [x={ndocs}, y={pat_30}] {ndocs_memory.dat};
\end_layout

\begin_layout Plain Layout


\backslash
addplot[mark=diamond] table [x={ndocs}, y={vpt}] {ndocs_memory.dat};
\end_layout

\begin_layout Plain Layout


\backslash
legend{kdt,pat ($n_c=10$),pat ($n_c=30$),vpt}
\end_layout

\begin_layout Plain Layout


\backslash
end{axis}
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Tree process memory usage.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The pa tree had a very memory usage since the principal axes was not sparse.
 A possible mitigation is the use of sparse principal component analysis.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Section
Bibliography
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "report"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
