#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman lmodern
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\columnsep 0.2in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Nearest Neighbor Algorithms for Text Classification
\end_layout

\begin_layout Author
Steven Xu (350256)
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $k$
\end_inset

-nearest-neighbor algorithm is a method for classification, where a test
 instance is classified by using the majority vote of the 
\begin_inset Formula $k$
\end_inset

 closest training examples in the feature space.
\end_layout

\begin_layout Standard
It can in particular be used for text classification, achieves good effectivenes
s comparable but less than that of support vector machines.
 It can also handle multiple classes directly, but a naive implementation
 has high computational time and memory requirements, which are linear to
 the number of training examples.
\end_layout

\begin_layout Standard
We give a review of some algorithms with faster average time, and compare
 them for their effectiveness on text classification.
 We will also investigate the effect of an inverted index on time and memory.
\end_layout

\begin_layout Section
Algorithms
\end_layout

\begin_layout Standard
A common approach to search for exact nearest neighbors is by organizing
 a set of test vertices in a tree structure, then using branch and bound
 to prune portions of the tree which are known to be non-optimal.
 Algorithms include the K-D tree 
\begin_inset CommandInset citation
LatexCommand cite
key "sproull_refinements_1991"

\end_inset

, the principal axis tree 
\begin_inset CommandInset citation
LatexCommand cite
key "mcnames_fast_2001"

\end_inset

, and the vantage point tree 
\begin_inset CommandInset citation
LatexCommand cite
key "yianilos_data_1993"

\end_inset

.
\end_layout

\begin_layout Standard
Faster queries compared to the naive algorithm on high-dimensional datasets
 often depend on a low intrinsic dimension 
\begin_inset CommandInset citation
LatexCommand cite
key "mcnames_fast_2001"

\end_inset

.
\end_layout

\begin_layout Standard
We give algorithms to find the single nearest point, but it is easy to find
 multiple nearest points by using a max-heap to store the best distances,
 and use the maximum distance in the heap for comparison during search.
\end_layout

\begin_layout Subsection
K-D Tree
\end_layout

\begin_layout Standard
A k-d tree is a space partitioning binary tree.
 Each node includes a pivot element and some distance bounds to aid in the
 search process.
\end_layout

\begin_layout Standard
There are many ways to choose the axis, the simplest way is to cycle it
 amongst the dimensions of the dataset, described below.
\end_layout

\begin_layout Subsubsection
Tree Construction
\end_layout

\begin_layout Standard
We start from the root node of the tree with the entire dataset of points,
 and select the first dimension of the dataset as the axis to partition
 the tree.
 The pivot is selected as the median of all points in the dataset, projected
 on the axis.
 The space is then partitioned in two, the points to the left and right
 of the pivot, assigned to the left and right child nodes.
 For each child, we also store the minimum distance a projected point has
 to the pivot, to aid in branch-and-bound search.
\end_layout

\begin_layout Standard
The process is then repeated for each child, with the axis selected as the
 one after the parent's axis.
 We continue until no points remain.
\end_layout

\begin_layout Standard
Each node in the tree stores the axis used, the pivot point, and the minimum
 distances to the pivot of the left and right child.
\end_layout

\begin_layout Subsubsection
Tree Search
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $q$
\end_inset

 be the query point.
 The distance between 
\begin_inset Formula $q$
\end_inset

 and the root node's pivot is computed, and update this as the current best
 distance 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Standard
For each child node, we check whether the points in it can be closer to
 
\begin_inset Formula $q$
\end_inset

 than 
\begin_inset Formula $d$
\end_inset

, by intersecting a hypersphere 
\begin_inset Formula $S(q,d)$
\end_inset

 with the separating hyperplane.
 The distance between 
\begin_inset Formula $q$
\end_inset

 and the hyperplane can be calculated by projecting the query point on the
 axis, and adding or subtracting the minimum distance.
 We have no intersections if this is greater than 
\begin_inset Formula $d$
\end_inset

, therefore the node can be pruned.
\end_layout

\begin_layout Standard
We repeat the process for each child node that isn't pruned.
\end_layout

\begin_layout Subsection
Principal Axis Tree
\end_layout

\begin_layout Subsection
Vantage Point Tree
\end_layout

\begin_layout Subsection
Inverted Index
\end_layout

\begin_layout Section
Experimental Setup
\end_layout

\begin_layout Standard
30,000 documents were used from the RCV1v2 document collection.
 To pre-process each document, the bag of words model was used with unit-length
 normalized tf-idf weights using the 
\begin_inset Formula $l_{2}$
\end_inset

 metric, with the following formulas:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
tf_{d,t} & =log(1+f_{d,t})\\
idf_{t} & =log(\frac{N}{f_{t}})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $f_{d,t}$
\end_inset

 is the number of times term 
\begin_inset Formula $t$
\end_inset

 appears in the document 
\begin_inset Formula $d$
\end_inset

; 
\begin_inset Formula $f_{t}$
\end_inset

is the number of documents term 
\begin_inset Formula $t$
\end_inset

 appears in throughout the collection; and 
\begin_inset Formula $N$
\end_inset

 is the number of documents in the collection.
\end_layout

\begin_layout Standard
Euclidean distance was used as the distance function.
\end_layout

\begin_layout Section
Experimental Results
\end_layout

\begin_layout Standard
# of dimensions vs.
 average nodes visited
\end_layout

\begin_layout Standard
# of documents vs.
 average nodes visited
\end_layout

\begin_layout Subsection
Principal Axis Tree
\end_layout

\begin_layout Standard
Not very good for text classification.
 None of the nodes in the tree were pruned from the document set.
 The main problem is that the document vectors are usually far away from
 the boundary point (XXX have a table to support this).
 A possible explanation for this is that documents usually contain only
 very few words for a given topic relative to the length of the document
 vector, with the topic here being the principal axis vector.
\end_layout

\begin_layout Standard
Has a very memory usage since the principal axis are not sparse.
 A possible mitigation is the use of sparse principal component analysis.
\end_layout

\begin_layout Section
Bibliography
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "report"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
