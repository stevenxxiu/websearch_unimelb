#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman lmodern
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\columnsep 0.2in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Nearest Neighbor Algorithms for Text Classification
\end_layout

\begin_layout Author
Steven Xu (350256)
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $k$
\end_inset

-nearest-neighbor algorithm is a method for classification, where a test
 instance is classified by using the majority vote of the 
\begin_inset Formula $k$
\end_inset

 closest training examples in the feature space.
\end_layout

\begin_layout Standard
It can in particular be used for text classification, achieves good effectivenes
s comparable but less than that of support vector machines.
 It can also handle multiple classes directly, but a naive implementation
 has high computational time and memory requirements, which are linear to
 the number of training examples.
\end_layout

\begin_layout Standard
We give a review of some algorithms with faster average time, and compare
 them for their effectiveness on text classification.
\end_layout

\begin_layout Section
Algorithms
\end_layout

\begin_layout Standard
A common approach to search for exact nearest neighbors is by organizing
 points in the database into a tree structure, then using branch and bound
 to prune portions of the tree which are known to be non-optimal.
 Algorithms include the K-D tree 
\begin_inset CommandInset citation
LatexCommand cite
key "sproull_refinements_1991"

\end_inset

, the principal axis tree 
\begin_inset CommandInset citation
LatexCommand cite
key "mcnames_fast_2001"

\end_inset

, and the vantage point tree 
\begin_inset CommandInset citation
LatexCommand cite
key "yianilos_data_1993"

\end_inset

.
\end_layout

\begin_layout Standard
Faster-than-naive algorithms on high-dimensional datasets often depend on
 a low intrinsic dimension 
\begin_inset CommandInset citation
LatexCommand cite
key "mcnames_fast_2001"

\end_inset

.
\end_layout

\begin_layout Standard
We only give algorithms to find the single nearest point, but it is easy
 to find multiple nearest points by using a max-heap to store the best distances
, then using the maximum distance in the heap for comparison during search.
\end_layout

\begin_layout Subsection
K-D Tree
\end_layout

\begin_layout Standard
A k-d tree is a space partitioning binary tree, with an axis selected at
 each node to partition it's set of points.
 There are many ways to choose the axis, the simplest way is to cycle it
 amongst the dimensions of the dataset, described below.
\end_layout

\begin_layout Subsubsection
Tree Construction
\end_layout

\begin_layout Standard
We start from the root node of the tree with the entire dataset of points,
 and select the first dimension of the dataset as the axis to partition
 the tree.
 The pivot is selected as the median of all points in the dataset, projected
 on the axis.
 The space is then partitioned in two, the points to the left and right
 of the pivot, assigned to the left and right child nodes.
 For each child 
\begin_inset Formula $i$
\end_inset

, we also store the minimum distance a projected point has to the pivot,
 
\begin_inset Formula $lb_{i}$
\end_inset

, to aid in branch-and-bound search.
\end_layout

\begin_layout Standard
The process is then repeated for each child, with the axis selected as the
 one after the parent's axis.
 We continue until no points remain.
\end_layout

\begin_layout Standard
Each node stores the axis used, the pivot point, and the minimum distances
 to the pivot of the left and right child.
\end_layout

\begin_layout Subsubsection
Tree Search
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $q$
\end_inset

 be the query point.
 The distance between 
\begin_inset Formula $q$
\end_inset

 and the root node's pivot is computed, and update this as the current best
 distance 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Standard
For each child node 
\begin_inset Formula $i$
\end_inset

, we check whether the points in it can be closer to 
\begin_inset Formula $q$
\end_inset

 than 
\begin_inset Formula $d$
\end_inset

, by intersecting a hypersphere 
\begin_inset Formula $S(q,d)$
\end_inset

 with the region's separating hyperplane.
 The distance between 
\begin_inset Formula $q$
\end_inset

 and the hyperplane can be calculated by projecting the query point on the
 axis, computing it's distance to the pivot point, then adding 
\begin_inset Formula $lb_{i}$
\end_inset

.
 We have no intersections if this is greater than 
\begin_inset Formula $d$
\end_inset

, therefore the node can be pruned.
\end_layout

\begin_layout Standard
This is repeated until we have searched or eliminated all nodes.
\end_layout

\begin_layout Subsection
Principal Axis Tree
\end_layout

\begin_layout Standard
A principal axis tree is similar to that of a k-d tree, where the axis are
 selected to be the principal components of the dataset.
 This splits the data amongst the axis of most variance into a predefined
 
\begin_inset Formula $n_{c}$
\end_inset

 number of regions, to try to adapt and cluster the points in the dataset.
\end_layout

\begin_layout Subsubsection
Tree Construction
\end_layout

\begin_layout Standard
PCA is first performed on the dataset to compute the principal axis.
\end_layout

\begin_layout Standard
Each point in the dataset is then projected onto the first principal axis,
 and split evenly into 
\begin_inset Formula $n_{c}$
\end_inset

 regions, with the left and right separating hyperplanes for each region
 stored as projected points.
 This process is continued recursively until we have less than 
\begin_inset Formula $n_{c}$
\end_inset

 points in a node, where it then becomes a terminal node.
\end_layout

\begin_layout Standard
Each node stores the principal axis, and the left and right separating hyperplan
es for each region.
\end_layout

\begin_layout Subsubsection
Tree Search
\end_layout

\begin_layout Standard
The depth-first search begins by projecting the query point onto the first
 principal axis, with a binary search to determine which region it is in.
 This process is repeated for child region until we reach a terminal node,
 where we compute the distance and store it as the current best.
 The algorithm then moves onto the parent node, where the elimination criterion
 is applied to the siblings of the terminal node.
\end_layout

\begin_layout Standard
This continues until we have searched or eliminated all nodes.
\end_layout

\begin_layout Standard
Regions closer to the one the query point is in are therefore visited first,
 to arrive at a smaller distance sooner.
\end_layout

\begin_layout Standard
The elimination criterion for the root regions compares it's closest separating
 hyperplane's distance to the query point, and the current best distance,
 same as a k-d tree.
 For a child region 
\begin_inset Formula $r_{n}$
\end_inset

 nested in parent regions 
\begin_inset Formula $r_{n-1},r_{n-2},\dots,r_{1}$
\end_inset

, it can be proved, using the law of cosines 
\begin_inset CommandInset citation
LatexCommand cite
key "mcnames_fast_2001"

\end_inset

, that for any point 
\begin_inset Formula $x\in r_{n}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
d(q,x)^{2}\ge d(q,r_{1})^{2}+d(b_{2},r_{2})^{2}+\dots+d(b_{n},r_{n})^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $d(q,r_{i})^{2}$
\end_inset

 is the distance between 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $r_{i}$
\end_inset

's closest separating hyperplane, and 
\begin_inset Formula $b_{2}$
\end_inset

 is 
\begin_inset Formula $q$
\end_inset

 projected onto 
\begin_inset Formula $r_{1}$
\end_inset

's closest separating hyerplane, 
\begin_inset Formula $b_{3}$
\end_inset

 is 
\begin_inset Formula $b_{2}$
\end_inset

 projected onto 
\begin_inset Formula $r_{2}$
\end_inset

's separating hyperplane, and so forth.
\end_layout

\begin_layout Standard
If this inequality is not satisfied, the region can be pruned.
 For efficiency, we only need to compute the squared distance to every region.
\end_layout

\begin_layout Subsection
Vantage Point Tree
\end_layout

\begin_layout Standard
Both the k-d tree and the principal axis tree require Euclidean distances.
 The vantage point tree is a binary tree which can work with any metric.
 Cosine distance in particular can be turned into a metric.
\end_layout

\begin_layout Standard
Instead of using hyerplanes to partition the data, vantage point trees use
 spheres around a vantage point to partition data.
\end_layout

\begin_layout Standard
It has the theoretical property that a data distribution satisfying the
 Zero Point Spheres (ZPS) property (the probability of points being on a
 sphere is zero), and with a small enough distance bound, the nearest neighbor
 can be expected to be computed in 
\begin_inset Formula $O(log(n))$
\end_inset

 time with a database of size 
\begin_inset Formula $n$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "yianilos_data_1993"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Tree Construction
\end_layout

\begin_layout Standard
Starting from the root node, a vantage point 
\begin_inset Formula $p$
\end_inset

 is chosen amongst the points in the database 
\begin_inset Formula $S$
\end_inset

.
 This can be random for speed, but there can be other criterions to ensure
 that the tree is more balanced 
\begin_inset CommandInset citation
LatexCommand cite
key "yianilos_data_1993"

\end_inset

.
\end_layout

\begin_layout Standard
We then define a sphere with radius 
\begin_inset Formula $\mu=Median_{s\in S}d(p,s)$
\end_inset

.
 The points in the node are partitioned in two, with the left child being
 inside and right child outside or on the sphere.
 This can be done by computing the distances between each point and 
\begin_inset Formula $p$
\end_inset

 and 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mu$
\end_inset

.
 For each partition, upper and lower distance bounds to 
\begin_inset Formula $p$
\end_inset

 are computed.
 This is performed recursively for each child node.
\end_layout

\begin_layout Standard
Each node stores 
\begin_inset Formula $p$
\end_inset

, 
\begin_inset Formula $\mu$
\end_inset

, and the bounds.
\end_layout

\begin_layout Subsubsection
Tree Search
\end_layout

\begin_layout Standard
The distance between the root node's pivot and the query is stored as the
 current best.
 The node visiting criterion then prefers to visit the left node if this
 is closer to the left child, or right node otherwise.
 A node is visited if the distance is within it's bounds.
\end_layout

\begin_layout Standard
This is repeated until we have searched or eliminated all nodes.
\end_layout

\begin_layout Section
Experimental Setup
\end_layout

\begin_layout Standard
30,000 documents were used from the RCV1v2 document collection.
 To pre-process each document, the bag of words model was used with unit-length
 normalized tf-idf weights using the 
\begin_inset Formula $l_{2}$
\end_inset

 metric, with the following formulas:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
tf_{d,t} & =log(1+f_{d,t})\\
idf_{t} & =log(\frac{N}{f_{t}})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $f_{d,t}$
\end_inset

 is the number of times term 
\begin_inset Formula $t$
\end_inset

 appears in the document 
\begin_inset Formula $d$
\end_inset

; 
\begin_inset Formula $f_{t}$
\end_inset

is the number of documents term 
\begin_inset Formula $t$
\end_inset

 appears in throughout the collection; and 
\begin_inset Formula $N$
\end_inset

 is the number of documents in the collection.
\end_layout

\begin_layout Standard
Euclidean distance was used as the distance function, which is rank-equivalent
 to cosine similarity due to unit-length normalization.
\end_layout

\begin_layout Standard
We implemented all of the above algorithms in python.
 Due to python's performance limits compared to other languages, we evaluated
 performance based on the number of distances computed instead of the total
 time spent.
\end_layout

\begin_layout Standard
Partial distance search were not used due to it's negligble impact on performanc
e.
\end_layout

\begin_layout Section
Experimental Results
\end_layout

\begin_layout Standard
# of dimensions vs.
 average nodes visited
\end_layout

\begin_layout Standard
# of documents vs.
 average nodes visited
\end_layout

\begin_layout Subsection
Principal Axis Tree
\end_layout

\begin_layout Standard
Not very good for text classification.
 None of the nodes in the tree were pruned from the document set.
 The main problem is that the document vectors are usually far away from
 the boundary point (XXX have a table to support this).
 A possible explanation for this is that documents usually contain only
 very few words for a given topic relative to the length of the document
 vector, with the topic here being the principal axis vector.
\end_layout

\begin_layout Standard
Has a very memory usage since the principal axis are not sparse.
 A possible mitigation is the use of sparse principal component analysis.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Section
Bibliography
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "report"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
